##Encoder-Decoder_Case_Study

This is a case study i lead in Spring 2023 for CS7643 Deep Learning Class. In this study, we disscussed the pros and cons of fine tuning classic "deep" CNN frameworks for a image annotation task for Microsoft COCO dataset. 
We adepted encoder-decoder model framework from https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning by @sgrvinod. For the encoder part, we utilize VGG13, Resnet101, InceptionV3 as toys CNN for the encoder portion. These models was further cutted off the classification part to only use as a encoder. Their stock and fine tuned (on COCO) version were compared in the annotation task. For the LSTM decoder, we tested whether using a Glover6B weight is a good initalization stratgy or not. Full report can be found as report.pdf.

To actually use code here, you need to download the COCO dataset 2014 Train (http://images.cocodataset.org/zips/train2014.zip) and 2014 Val (http://images.cocodataset.org/zips/val2014.zip) dataset. For actual training/val/testing, the karpathy's spilted was used as documented in https://www.kaggle.com/datasets/shtvkumar/karpathy-splits.

Abstract:
The Encoder-Decoder model is a fundamental component of machine learning that allows arbitrary input forms to be encoded into arbitrary output forms. However, the flexibility of the Encoder-Decoder model comes with extra complexity compared to traditional machine learning models, making it difficult to train. To address this difficulty in training, researchers commonly use a pre-trained encoder such as the CNN model. This paper builds an Encoder-Decoder network for Microsoft COCO image captioning using three distinct CNN networks - VGG16, Resnet101, and InceptionV3 - as encoders. The ImageNet pre-trained weights of these CNNs were fine-tuned on the Microsoft COCO dataset for a multi-label classification task. The paper compares the feature extraction quality of the pre-trained CNNs and the fine-tuned CNNs for image captioning tasks by training an LSTM decoder network on top of each CNN to generate captions. The paper also tests the initialization quality of the GloVe 6B 200 dimension embedding for all the Encoder-Decoder networks in this work. The results provide a cost and benefit analysis of utilizing pre-trained CNN or embedding for image captioning tasks. By comparing the performance of pre-trained and fine-tuned CNNs, the paper aims to determine whether the additional training time required for fine-tuning is worth the improved performance for the target task. We found that fine tuning only improve performance of VGG16 encoder configuration's image caption ability.
